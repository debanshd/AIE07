{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ü§ù Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ü§ù Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ü§ù Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Date received\", \n",
        "      \"Product\", \n",
        "      \"Sub-product\", \n",
        "      \"Issue\", \n",
        "      \"Sub-issue\", \n",
        "      \"Consumer complaint narrative\", \n",
        "      \"Company public response\", \n",
        "      \"Company\", \n",
        "      \"State\", \n",
        "      \"ZIP code\", \n",
        "      \"Tags\", \n",
        "      \"Consumer consent provided?\", \n",
        "      \"Submitted via\", \n",
        "      \"Date sent to company\", \n",
        "      \"Company response to consumer\", \n",
        "      \"Timely response?\", \n",
        "      \"Consumer disputed?\", \n",
        "      \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loan_complaint_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    loan_complaint_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"LoanComplaints\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, the most common issues with loans appear to be related to mismanagement and errors by loan servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, inaccuracies on credit reports, and difficulties in applying payments correctly. These problems often lead to incorrect reporting, increased balances, and increased financial hardship for borrowers.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, yes, some complaints did not get handled in a timely manner. Specifically, there are complaints where the response was marked as \"No\" for timely response:\\n\\n- Complaint ID 12709087 (MOHELA) received on 03/28/25, where the response was \"Closed with explanation\" and marked as \"Timely response?\": No.\\n- Complaint ID 12832400 (Maximus Federal Services, Inc.) received on 04/05/25, which was marked as \"Timely response?\": Yes.\\n- Complaint ID 12973003 (EdFinancial Services) received on 04/14/25, and marked as \"Timely response?\": Yes.\\n- Complaint ID 12975634 (Maximus Federal Services, Inc.) received on 04/14/25, marked as \"Timely response?\": Yes.\\n- Complaint ID 13062402 (Nelnet, Inc.) received on 04/18/25, marked as \"Timely response?\": Yes.\\n- Complaint ID 13056764 (EdFinancial Services) received on 04/18/25, marked as \"Timely response?\": Yes.\\n- Several other complaints, such as the one received on 04/24/25 (Maximus) with response \"Closed with explanation\" and marked as \"Timely response?\": Yes.\\n- The complaint on 04/27/25 (Nelnet) was marked as \"Timely response?\": Yes.\\n\\nHowever, the complaint from 03/28/25 (Complaint ID 12709087) shows that the consumer dispute was not handled in a timely manner, as indicated by \"Timely response?\": No.\\n\\nIn summary, at least one complaint was not handled in a timely manner.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors highlighted in the complaints:\\n\\n1. **Lack of Clear Communication and Notification:** Many borrowers were not properly notified about important changes such as loan transferences, when repayment was to start, or updates on payment plans. For example, some complain about loans being transferred without notice, or being expected to start payments before the grace period without explanation.\\n\\n2. **Complex and Hidden Interest Accumulation:** Borrowers expressed difficulty understanding how interest accumulates, especially when loans are placed in forbearance or deferment, leading to continued interest that negates their payments and increases overall debt.\\n\\n3. **Inadequate or Restrictive Payment Options:** Many complain that the available options, such as forbearance or deferment, either extend the repayment period or cause interest to grow significantly. Some describe the inability to make additional payments directly toward principal, which prolongs the debt.\\n\\n4. **Financial Hardships and Economic Hardship:** Over time, borrowers have faced hardships like unemployment, fixed or stagnant wages, or other financial struggles that make it difficult to keep up with payments. For example, some have lost jobs or are unable to increase payments without jeopardizing their basic needs.\\n\\n5. **Mismanagement and Poor Servicing:** Several complaints indicate frustration with loan servicers mismanaging accounts, providing incorrect or conflicting information, or failing to address delinquency notices properly. This mismanagement can lead to unintentional missed payments or credit report errors, which further complicate repayment efforts.\\n\\n6. **Perceived Unfair Practices and Lack of Transparency:** Borrowers feel misled about the terms of their loans, interest rates, or options for loan forgiveness. Some have had their credit negatively impacted due to lack of notification or errors in reporting.\\n\\n7. **Difficulty Accessing Income-Driven or Forgiveness Programs:** Many mention being unable to effectively access income-based repayment or forgiveness programs, or being misinformed about their eligibility, which affects their ability to manage or reduce their debt.\\n\\nIn summary, the failure to pay back loans is often due to a combination of inadequate communication, complex interest calculations, limited flexible repayment options, and economic hardships, compounded by issues with loan servicer management.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, such as disputes over fees, repayment practices, or inaccurate or confusing information about loans. Specifically, complaints often involve difficulty applying payments correctly, lack of clear information about loan balances or terms, and issues with the accuracy of loan data or repayment calculations.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, all of the complaints listed indicate that the companies responded in a \"timely\" manner, with responses marked as \"Yes\" under the \"Timely response?\" field. Therefore, it appears that no complaints were left unresolved or handled outside of the appropriate timeframe.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People often fail to pay back their loans due to a variety of reasons, including issues with payment plans, miscommunication, or administrative errors. In the provided context, specific reasons include:\\n\\n- Problems with payment plans, such as being steered into the wrong types of forbearances or experiencing complications with their repayment options.\\n- Lack of communication from loan servicers, such as not receiving responses to requests for forbearance or deferment, or being unaware of transfer or changes in loan servicing.\\n- Errors or glitches in payment processing, such as payments being reversed or not properly enrolled in autopay, leading to missed payments and negative credit impacts.\\n- Deception or mismanagement by loan servicers, which can include selling loans to entities that refuse to help or providing bad information about loan status.\\n- Lack of proper notification or transparency, which results in borrowers being unaware of their repayment obligations or account status, especially when contacts are made via email or spam folders.\\n\\nOverall, failures to pay back loans often stem from administrative errors, poor communication, or mismanagement by loan servicers, compounded by borrowers' lack of awareness or understanding of their loan status.\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### ‚úÖ Answer:\n",
        "\n",
        "Here's an example query where BM25 would likely perform better than embeddings:\n",
        "\n",
        "## Example Query:\n",
        "**\"What are the specific fees charged by Nelnet for late payments?\"**\n",
        "\n",
        "## Why BM25 would be better:\n",
        "\n",
        "### 1. **Exact Keyword Matching**\n",
        "- BM25 excels at finding documents containing specific terms like \"Nelnet\", \"fees\", \"late payments\"\n",
        "- It would prioritize documents that contain these exact words, even if they're not semantically similar\n",
        "- Embeddings might miss documents that use different phrasing (e.g., \"penalties\" instead of \"fees\")\n",
        "\n",
        "### 2. **Company/Entity Names**\n",
        "- \"Nelnet\" is a specific company name that BM25 would treat as a high-value term\n",
        "- Embeddings might group it with other loan servicers, diluting the relevance\n",
        "- BM25's bag-of-words approach gives equal weight to proper nouns\n",
        "\n",
        "### 3. **Technical/Specific Terminology**\n",
        "- Terms like \"late payments\", \"fees\", \"charges\" are specific financial terms\n",
        "- BM25 would find documents containing these exact phrases\n",
        "- Embeddings might retrieve documents about general payment issues instead\n",
        "\n",
        "### 4. **Sparse Information Retrieval**\n",
        "- When looking for specific factual information (like fee amounts), BM25's keyword-based approach is more precise\n",
        "- Embeddings might return broader, more general documents about payment problems\n",
        "\n",
        "### 5. **Domain-Specific Vocabulary**\n",
        "- Financial/loan terminology often has precise meanings\n",
        "- BM25 treats each term independently, while embeddings might conflate related but distinct concepts\n",
        "- For example, \"late payment fees\" vs \"processing fees\" vs \"origination fees\" are different concepts\n",
        "\n",
        "## Real-world scenario:\n",
        "If a user is specifically looking for Nelnet's late payment fee structure, BM25 would likely return documents that explicitly mention \"Nelnet\" and \"late payment fees\", while embeddings might return documents about general payment issues with any servicer.\n",
        "\n",
        "This demonstrates BM25's strength in **precision** for specific, keyword-heavy queries versus embeddings' strength in **semantic understanding** for conceptual queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the most common issue with loans appears to be problems related to improper handling and misinformation by lenders or servicers. Specific issues include errors in loan balances, misapplied payments, wrongful denials of payment plans, lack of clear communication, and mishandling of loan data. These problems often lead to increased debt despite payments, disputes over account accuracy, and privacy violations.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, some complaints were handled in a timely manner, as indicated by the responses marked \"Yes\" for \"Timely response?\" However, there are also complaints where issues remain unresolved or lengthy delays are noted, such as the complaint about pending response after over a year and nearly 18 months with no resolution. \\n\\nTherefore, while some complaints did receive timely responses, others experienced significant delays or have not yet been resolved.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors highlighted in the complaints:\\n\\n1. Lack of understanding and communication: Borrowers were often not informed about their repayment obligations or the details of their loans, including interest accumulation and repayment terms.\\n\\n2. Accumulation of interest and stagnant wages: Interest continued to grow even when payments were deferred or reduced, making loans harder to pay off over time. Borrowers also faced stagnant wages, which made increasing payments difficult.\\n\\n3. Limited repayment options: Many borrowers felt that available options, such as forbearance or deferment, extended the repayment period and increased total interest, without providing manageable solutions.\\n\\n4. Financial hardship and lack of support: Borrowers, especially young or low-income individuals, struggled with balancing loan repayment alongside everyday expenses, leading to difficulties in making payments.\\n\\n5. Administrative issues and mismanagement: There were reports of incorrect account information, unauthorized loan transfers, and poor communication from lenders or servicers, which contributed to confusion and inability to keep up with repayment.\\n\\nOverall, a combination of economic challenges, inadequate information, and administrative problems contributed to the failure to pay back loans.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be problems related to mismanagement by loan servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and inadequate communication or transparency. Many complaints highlight errors in account handling, improper reporting, and disputes over loan terms and balances.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided complaints data, it appears that several complaints indicate issues with timely handling, response, or resolution of complaints or issues by the companies involved. For example:\\n\\n- One complaint (row 441) indicates the response was **not timely** (\"Timely response?\": \"No\").\\n- Multiple complaints report long delays or failure to respond within expected timeframes, including complaints from rows 66, 90, 95, and others, mentioning delays of over 30 days, months, or even years without resolution or proper handling.\\n- Several complaints explicitly mention that the companies failed to address issues promptly or did not respond at all, indicating that complaints were not handled in a timely manner.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner, as evidenced by multiple complaints stating delays, lack of response, or overdue resolutions.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily because of several interconnected reasons highlighted in the complaints:\\n\\n1. **Accumulation of Unmanageable Interest:** Many borrowers were not adequately informed that interest would continue to accrue and compound during forbearance or deferment, leading to an increasing balance that becomes difficult to repay.\\n\\n2. **Lack of Clear Communication and Guidance:** Borrowers often received little to no notice or explanations about their repayment obligations, changes in loan servicing, or available repayment options such as income-driven plans or loan rehabilitation.\\n\\n3. **Predatory and Misleading Practices:** Some borrowers experienced forbearance steering, where they were repeatedly placed into long-term forbearances without being informed about more sustainable options, causing their balances to balloon.\\n\\n4. **Systemic Service Failures:** Errors in loan balances, misapplied payments, improper transfers between servicers, and inaccurate reporting to credit bureaus created confusion and negative credit impacts, making repayment more challenging.\\n\\n5. **Financial Hardship and Lack of Support:** Unexpected events like unemployment, illness, homelessness, or underemployment made regular payments impossible, especially when servicers failed to offer workable alternatives.\\n\\n6. **Limited Access to Forgiveness or Discharge Programs:** Many borrowers did not qualify for loan forgiveness programs and were misled about available options, trapping them in unaffordable repayment plans.\\n\\nIn summary, failure to pay back loans was often due to complex systemic issues, misleading servicing practices, lack of proper guidance, and borrowers‚Äô financial hardships, which were exacerbated by unresponsiveness or negligence on the part of loan servicers.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### ‚úÖ Answer:\n",
        "\n",
        "Here's how generating multiple reformulations of a user query can improve recall:\n",
        "\n",
        "## How Multiple Query Reformulations Improve Recall:\n",
        "\n",
        "### 1. **Addressing Vocabulary Mismatch**\n",
        "- **Problem**: Users and documents may use different terms for the same concept\n",
        "- **Solution**: Multiple reformulations can include synonyms, related terms, and alternative phrasings\n",
        "- **Example**: \n",
        "  - Original: \"loan payment problems\"\n",
        "  - Reformulations: \"payment difficulties\", \"repayment issues\", \"trouble with payments\", \"payment challenges\"\n",
        "\n",
        "### 2. **Capturing Different Query Intentions**\n",
        "- **Problem**: A single query might not capture all possible interpretations\n",
        "- **Solution**: Different reformulations can explore various aspects of the query\n",
        "- **Example**:\n",
        "  - Original: \"student loan issues\"\n",
        "  - Reformulations: \n",
        "    - \"problems with student loans\"\n",
        "    - \"student loan complaints\"\n",
        "    - \"difficulties with education financing\"\n",
        "    - \"issues with federal student aid\"\n",
        "\n",
        "### 3. **Handling Query Ambiguity**\n",
        "- **Problem**: Queries can be ambiguous or underspecified\n",
        "- **Solution**: Multiple reformulations can disambiguate and explore different meanings\n",
        "- **Example**:\n",
        "  - Original: \"loan problems\"\n",
        "  - Reformulations:\n",
        "    - \"problems with loan payments\"\n",
        "    - \"problems with loan servicers\"\n",
        "    - \"problems with loan applications\"\n",
        "    - \"problems with loan forgiveness\"\n",
        "\n",
        "### 4. **Expanding Semantic Coverage**\n",
        "- **Problem**: Documents might be relevant but use different semantic expressions\n",
        "- **Solution**: Reformulations can cover broader semantic space\n",
        "- **Example**:\n",
        "  - Original: \"late payment fees\"\n",
        "  - Reformulations:\n",
        "    - \"penalties for missed payments\"\n",
        "    - \"charges for overdue amounts\"\n",
        "    - \"fees for delayed payments\"\n",
        "    - \"consequences of not paying on time\"\n",
        "\n",
        "### 5. **Overcoming Embedding Limitations**\n",
        "- **Problem**: Embeddings might miss relevant documents due to semantic drift\n",
        "- **Solution**: Multiple queries increase the chance of finding relevant documents\n",
        "- **Mechanism**: Each reformulation creates a different vector, potentially retrieving different document sets\n",
        "\n",
        "## Technical Process:\n",
        "\n",
        "1. **Query Generation**: LLM creates multiple reformulations of the original query\n",
        "2. **Parallel Retrieval**: Each reformulation is used to retrieve documents independently\n",
        "3. **Deduplication**: Remove duplicate documents across all retrieval results\n",
        "4. **Ranking**: Combine and rank all unique documents using fusion algorithms (like Reciprocal Rank Fusion)\n",
        "\n",
        "## Benefits:\n",
        "\n",
        "- **Higher Recall**: More relevant documents are found across different query formulations\n",
        "- **Better Coverage**: Documents that might be missed by a single query are captured\n",
        "- **Robustness**: Less dependent on the specific wording of the original query\n",
        "- **Comprehensive Results**: Provides a more complete picture of available relevant information\n",
        "\n",
        "This approach essentially \"casts a wider net\" by exploring the query space from multiple angles, significantly improving the chances of finding all relevant documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = loan_complaint_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the provided complaints, appears to be related to \"Dealing with your lender or servicer,\" including specific problems such as discrepancies in loan balances, misapplied payments, wrongful denials of payment plans, and issues with loan reporting or verification. These issues reflect systemic problems in loan servicing and communication between lenders and borrowers.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, several complaints indicate that they were not handled in a timely manner. Specifically:\\n\\n- The complaint regarding the student loan application submitted to MOHELA found that the consumer was told it would take 15 days for someone to contact them, but they had not heard back as of the date of the complaint, which was after that period. Additionally, repeated calls with wait times of four hours or more suggest delays in handling the issue.\\n- Another complaint involved waiting on hold for several hours (up to seven hours at one point) with MOHELA, indicating significant delays in resolving issues.\\n- The complaint about disputes sent to credit bureaus over 30 days ago also remained unresolved.\\n\\nIn summary, yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans primarily due to a variety of challenges including financial hardship, mismanagement, lack of proper information, and issues related to the status of their lenders or educational institutions. Specific reasons mentioned include:\\n\\n- Experiencing severe financial hardship after graduation, making it difficult to make or maintain payments.\\n- Being misled or inadequately informed about the long-term financial consequences of taking out loans, especially when attending schools facing financial instability.\\n- The inability to secure employment in their field of study, which hampers their ability to repay loans.\\n- Withdrawal of communication or failure of loan servicers to provide clear information and proper notification regarding repayment obligations.\\n- Mistakes or issues with credit reporting and collection practices, which can complicate repayment efforts.\\n\\nOverall, these factors combined can hinder individuals' ability to successfully repay their loans.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common issue with loans, based on the complaints provided, appears to be problems related to the handling and management of student loans by servicers. The prevalent issues include:\\n\\n- **Dealing with your lender or servicer** (e.g., receiving bad information, improper handling of loan accounts, problems with paying or applying payments, and inappropriate loan transfers)\\n- **Trouble with how payments are being handled** (e.g., inability to apply payments correctly, interest capitalization, or misapplication of payments)\\n- **Incorrect or misleading information about loan balances and status** (such as inaccurate reporting, hidden or unexplained interest growth, or errors in loan details)\\n- **Lack of transparency and communication** (including failure to inform borrowers of loan transfer, default status, or changes in loan terms)\\n- **Problems with loan classification or mismanagement** (e.g., incorrect loan type classification, improper ending of deferments, or mishandling of consolidation processes)\\n\\nWhile issues also involve bad information, fee disputes, and fraud concerns, the most recurring theme centers around poor servicer practices, mismanagement, and lack of clear communication, which negatively impact borrower understanding and repayment process.\\n\\n**In summary:**  \\n**The most common issue with loans is poor management and handling by loan servicers, leading to misinformation, misapplication of payments, and lack of transparency.**'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, yes, there were complaints that did not get handled in a timely manner. Specifically:\\n\\n- Complaint ID 12709087 received on 03/28/25 was marked as \"No\" for timely response.\\n- Complaint ID 12935889 received on 04/11/25 was also marked as \"No\" for timely response.\\n- Complaint ID 12668396 received on 03/26/25 was marked as \"No\" for timely response.\\n\\nMost other complaints indicate responses that were timely (\"Yes\") or are not explicitly marked as late responses, but at least these three clearly indicate delays in handling. \\n\\nSo, the answer is: Yes, some complaints did not get handled in a timely manner.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People failed to pay back their loans primarily due to a combination of factors highlighted in the complaints:\\n\\n1. **Lack of Clear or Accurate Information**: Borrowers were often misinformed or insufficiently informed about their repayment options, loan balances, interest accrual, and the status of their loans, which led to confusion and unintentional delinquency.\\n\\n2. **Predatory or Coercive Servicing Practices**: Many borrowers experienced steering into long-term forbearances, default, or consolidation without being informed of alternative options like income-driven repayment plans or loan rehabilitation. This practice, known as \"forbearance steering,\" increased their total debt due to accumulated interest and reduced their chances of manageable repayment.\\n\\n3. **Unfair Collection and Reporting Practices**: Complaints include improper reporting of delinquency, failure to notify borrowers of missed payments, and misreporting loan statuses, which damaged credit scores and hindered repayment efforts.\\n\\n4. **Interest Capitalization and Accumulation**: Borrowers faced escalating balances because of unchecked interest that continued to grow during forbearance, deferment, or mismanagement, making the loans increasingly unpayable.\\n\\n5. **Insufficient Customer Support and Communication Failures**: Many reports describe difficulty reaching servicers, unresponsive or misleading customer service, and failure to notify borrowers about loan transfers, upcoming payments, or delinquency status, leading to late payments and default.\\n\\n6. **Mismanagement and Mishandling by Loan Servicers**: Errors in balances, misapplied payments, or not maintaining accurate records caused confusion and additional hardship, preventing borrowers from effectively managing or repaying their loans.\\n\\n7. **Economic Hardship and Unforeseen Difficulties**: Borrowers faced financial hardships due to stagnant wages, unemployment, or other financial crises, which made it difficult to meet repayment obligations, especially when options for relief were not properly communicated.\\n\\nIn summary, systemic issues, miscommunication, aggressive collection tactics, and the burden of compounded interest contributed significantly to why people failed to pay back their loans.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common issue with loans appears to be related to problems with repayment and servicing, such as difficulties in making payments, inaccurate billing or payment amounts, and lack of clear communication from loan servicers. Many complaints mention issues like auto-debit problems, incorrect interest calculations, and delays or failures in processing recertifications or payment plans. These recurring themes suggest that mismanagement or miscommunication regarding loan repayment is a prevalent concern among borrowers.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, most complaints indicate that responses from the companies were marked as \"Closed with explanation\" and are noted as being handled in a timely manner (\"Yes\" for timely response). However, one complaint explicitly states that the company, Nelnet, did not respond to the complaint‚Äîdespite multiple attempts‚Äîand the complaint remains unresolved with no proper handling mentioned.\\n\\nSpecifically:\\n- The complaint regarding the transferred account with alleged misconduct (Complaint ID: 13331376) shows that Nelnet never responded to the consumer\\'s letters, despite acknowledgment of receipt and multiple follow-ups. The company\\'s response was \"Closed with explanation,\" but it appears that the complaints about misconduct and errors were not addressed properly or promptly.\\n\\n**Conclusion:**  \\nYes, there was at least one complaint that did not get handled in a timely manner, or potentially at all, as per the complaint narrative.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People failed to pay back their loans for various reasons, including issues with the processing or handling of their payments, misunderstandings about their loan status, difficulties with loan servicers providing inaccurate or incomplete information, and delays or errors in re-amortization or reactivation of payments after forbearance periods ended. Additionally, some borrowers faced disputes over the legitimacy of their debts or experienced improper reporting of their loan status, which contributed to non-repayment. In some cases, the lack of transparency, poor communication, or alleged deliberate stalling by loan servicers also hindered borrowers' ability to manage and repay their loans effectively.\""
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### ‚úÖ Answer:\n",
        "\n",
        "Here's how semantic chunking would behave with short, highly repetitive sentences (like FAQs) and how to adjust the algorithm:\n",
        "\n",
        "### 1. **Over-chunking Problem**\n",
        "- **Issue**: Short, repetitive sentences will have very similar embeddings\n",
        "- **Result**: The algorithm might create too many tiny chunks or fail to find meaningful breakpoints\n",
        "- **Example**: FAQ sentences like \"What is X?\" \"How do I Y?\" \"Where can I find Z?\" would all have similar semantic vectors\n",
        "\n",
        "### 2. **Poor Semantic Differentiation**\n",
        "- **Issue**: Repetitive content lacks semantic diversity\n",
        "- **Result**: The similarity thresholding methods (percentile, standard deviation, etc.) won't find clear breakpoints\n",
        "- **Problem**: All sentences appear equally similar, so no natural chunking boundaries are detected\n",
        "\n",
        "### 3. **Ineffective Thresholding**\n",
        "- **Issue**: Methods like percentile or standard deviation assume semantic variation\n",
        "- **Result**: With repetitive content, these methods may create arbitrary or inconsistent chunks\n",
        "- **Example**: If all sentence similarities are 0.85-0.90, percentile-based chunking becomes unreliable\n",
        "\n",
        "## How to Adjust the Algorithm:\n",
        "\n",
        "### 1. **Use Structural Chunking Instead**\n",
        "```python\n",
        "# Instead of semantic chunking, use rule-based chunking\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "structural_chunker = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. **Adjust Threshold Parameters**\n",
        "```python\n",
        "# Use more aggressive thresholding for repetitive content\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    percentile_threshold=95  # Higher threshold for more aggressive chunking\n",
        ")\n",
        "```\n",
        "\n",
        "### 3. **Pre-filter Repetitive Content**\n",
        "```python\n",
        "# Remove or consolidate repetitive sentences before chunking\n",
        "def deduplicate_sentences(documents):\n",
        "    seen_content = set()\n",
        "    filtered_docs = []\n",
        "    for doc in documents:\n",
        "        if doc.page_content not in seen_content:\n",
        "            seen_content.add(doc.page_content)\n",
        "            filtered_docs.append(doc)\n",
        "    return filtered_docs\n",
        "```\n",
        "\n",
        "### 4. **Use Hybrid Approach**\n",
        "```python\n",
        "# Combine semantic and structural chunking\n",
        "def hybrid_chunking(documents):\n",
        "    # First, use structural chunking for repetitive sections\n",
        "    structural_chunks = structural_chunker.split_documents(documents)\n",
        "    \n",
        "    # Then, apply semantic chunking only to non-repetitive sections\n",
        "    semantic_chunks = []\n",
        "    for chunk in structural_chunks:\n",
        "        if has_semantic_variation(chunk):\n",
        "            semantic_chunks.extend(semantic_chunker.split_documents([chunk]))\n",
        "        else:\n",
        "            semantic_chunks.append(chunk)\n",
        "    \n",
        "    return semantic_chunks\n",
        "```\n",
        "\n",
        "### 5. **Adjust Embedding Strategy**\n",
        "```python\n",
        "# Use domain-specific embeddings or fine-tuned models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Use a model fine-tuned for your specific domain\n",
        "domain_embeddings = SentenceTransformer('domain-specific-model')\n",
        "```\n",
        "\n",
        "### 6. **Implement Content-Aware Chunking**\n",
        "```python\n",
        "def content_aware_chunking(documents):\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        # Check if content is repetitive\n",
        "        if is_repetitive_content(doc.page_content):\n",
        "            # Use larger chunks for repetitive content\n",
        "            chunks.extend(structural_chunker.split_documents([doc]))\n",
        "        else:\n",
        "            # Use semantic chunking for varied content\n",
        "            chunks.extend(semantic_chunker.split_documents([doc]))\n",
        "    return chunks\n",
        "```\n",
        "\n",
        "## Best Practices for Repetitive Content:\n",
        "\n",
        "1. **Analyze content first**: Determine if semantic chunking is appropriate\n",
        "2. **Use structural chunking**: For FAQs and repetitive content, rule-based chunking is often better\n",
        "3. **Combine approaches**: Use semantic chunking only where it adds value\n",
        "4. **Adjust thresholds**: Use higher thresholds for repetitive content\n",
        "5. **Consider domain-specific solutions**: FAQ content might benefit from question-answer pair chunking\n",
        "\n",
        "## Alternative for FAQs:\n",
        "```python\n",
        "# For FAQ content, chunk by Q&A pairs instead\n",
        "def faq_chunking(documents):\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        # Split by question-answer patterns\n",
        "        qa_pairs = extract_qa_pairs(doc.page_content)\n",
        "        for qa in qa_pairs:\n",
        "            chunks.append(Document(page_content=qa, metadata=doc.metadata))\n",
        "    return chunks\n",
        "```\n",
        "\n",
        "The key insight is that semantic chunking works best with semantically diverse content. For repetitive content like FAQs, traditional structural chunking or domain-specific approaches are often more effective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ü§ù Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### üèóÔ∏è Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dataset URL: https://smith.langchain.com/datasets/332652ca-a3a1-4476-a487-0c7c76b566a5\n",
            " Dataset: goldendataset-20250729-033937\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 1. SETUP LANGSMITH DATASET\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "from getpass import getpass \n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from langsmith import Client\n",
        "\n",
        "# Setup LangSmith API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Please enter your LANGCHAIN API key!\")\n",
        "\n",
        "# Create LangSmith client\n",
        "client = Client()\n",
        "\n",
        "# Create dataset for evaluation\n",
        "dataset_name = f\"goldendataset-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Retrieval Methods Evaluation Dataset\"\n",
        ")\n",
        "\n",
        "print(f\" Dataset URL: https://smith.langchain.com/datasets/{langsmith_dataset.id}\")\n",
        "print(f\" Dataset: {dataset_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Loaded 825 loan complaint documents\n",
            "üìã Sample document: The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new...\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 2: LOAD DOCUMENTS\n",
        "# =============================================================================\n",
        "\n",
        "# Use the loan_complaint_data already loaded in the notebook\n",
        "print(f\"üìä Loaded {len(loan_complaint_data)} loan complaint documents\")\n",
        "print(f\"üìã Sample document: {loan_complaint_data[0].page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9b97e152a304e86810dd30f8f504692",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a0e998023864e608cb83946a6a717a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 94e6048c-7d08-4191-aebf-bba94ae7d4fa does not have a summary. Skipping filtering.\n",
            "Node baada44b-c39c-4e9f-8259-0ced2560097d does not have a summary. Skipping filtering.\n",
            "Node 3d5ac006-558d-43e2-a427-8a833f1ebff3 does not have a summary. Skipping filtering.\n",
            "Node 566b8b5b-e0be-4c64-9b5c-d806a643fed1 does not have a summary. Skipping filtering.\n",
            "Node 868c1e29-e765-4398-96c7-3d5e9ec0446a does not have a summary. Skipping filtering.\n",
            "Node c1086a1a-1175-40ef-a564-8b1ca5125bd2 does not have a summary. Skipping filtering.\n",
            "Node 585e26d7-9f08-4d07-842d-a332778a42ee does not have a summary. Skipping filtering.\n",
            "Node 14dc12b5-7570-452b-8a0c-332cb1b9ec6e does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "250752276634453a82661859fc4a71e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/52 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a405d7be40e4e5eb75a794dfb6b72a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79e42355ad104ca585aa7e2ff384dc23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c741d524ae24fafb5a677b818523e67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "026b47ae2a2a4755b182e3d65716fa59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Generated 10 questions\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How come XXXX XXXX just go and look at my stuf...</td>\n",
              "      <td>[XXXX XXXX violated my FERPA rights by accessi...</td>\n",
              "      <td>XXXX XXXX violated my FERPA rights by accessin...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Wut FCRA requriements were not folowed in this...</td>\n",
              "      <td>[I am filing this complaint against XXXX  due ...</td>\n",
              "      <td>Under the Fair Credit Reporting Act (FCRA), lo...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What issue occurred after the student loan was...</td>\n",
              "      <td>[Federal request for payment history and recor...</td>\n",
              "      <td>After the student loan was transferred to XXXX...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How FERPA got broke by Nelnet data breach?</td>\n",
              "      <td>[I am filing this complaint against Nelnet due...</td>\n",
              "      <td>The Nelnet data breach raised concerns over a ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Howw can a loan servicer's repeated payment re...</td>\n",
              "      <td>[I've made regular payments on my loan but the...</td>\n",
              "      <td>In the provided context, the borrower's credit...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>If a student loan was paid off as part of a bo...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nMy student loan was part of a borr...</td>\n",
              "      <td>When a student loan is paid off through a borr...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What was the impact of the Department of Gover...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nOn XX/XX/year&gt;, XXXX XXXX instruct...</td>\n",
              "      <td>The Department of Government Efficiency gained...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How have issues with NelNet's online account s...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI spoke with a representative on t...</td>\n",
              "      <td>Borrowers have faced significant challenges wi...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How did Nelnet's data breach and online accoun...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI am filing this complaint against...</td>\n",
              "      <td>Nelnet's data breach led to concerns about una...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How did the actions of the Department of Gover...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI am submitting this complaint due...</td>\n",
              "      <td>The compromise of personal information for stu...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How come XXXX XXXX just go and look at my stuf...   \n",
              "1  Wut FCRA requriements were not folowed in this...   \n",
              "2  What issue occurred after the student loan was...   \n",
              "3         How FERPA got broke by Nelnet data breach?   \n",
              "4  Howw can a loan servicer's repeated payment re...   \n",
              "5  If a student loan was paid off as part of a bo...   \n",
              "6  What was the impact of the Department of Gover...   \n",
              "7  How have issues with NelNet's online account s...   \n",
              "8  How did Nelnet's data breach and online accoun...   \n",
              "9  How did the actions of the Department of Gover...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [XXXX XXXX violated my FERPA rights by accessi...   \n",
              "1  [I am filing this complaint against XXXX  due ...   \n",
              "2  [Federal request for payment history and recor...   \n",
              "3  [I am filing this complaint against Nelnet due...   \n",
              "4  [I've made regular payments on my loan but the...   \n",
              "5  [<1-hop>\\n\\nMy student loan was part of a borr...   \n",
              "6  [<1-hop>\\n\\nOn XX/XX/year>, XXXX XXXX instruct...   \n",
              "7  [<1-hop>\\n\\nI spoke with a representative on t...   \n",
              "8  [<1-hop>\\n\\nI am filing this complaint against...   \n",
              "9  [<1-hop>\\n\\nI am submitting this complaint due...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  XXXX XXXX violated my FERPA rights by accessin...   \n",
              "1  Under the Fair Credit Reporting Act (FCRA), lo...   \n",
              "2  After the student loan was transferred to XXXX...   \n",
              "3  The Nelnet data breach raised concerns over a ...   \n",
              "4  In the provided context, the borrower's credit...   \n",
              "5  When a student loan is paid off through a borr...   \n",
              "6  The Department of Government Efficiency gained...   \n",
              "7  Borrowers have faced significant challenges wi...   \n",
              "8  Nelnet's data breach led to concerns about una...   \n",
              "9  The compromise of personal information for stu...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 3: GENERATE GOLDEN DATASET USING ABSTRACTED SDG\n",
        "# =============================================================================\n",
        "\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "\n",
        "import random\n",
        "sample_docs = random.sample(loan_complaint_data, min(50, len(loan_complaint_data)))\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(sample_docs[:20], testset_size=10)\n",
        "\n",
        "# Extract questions\n",
        "test_questions = list(testset.to_pandas()['user_input'])\n",
        "print(f\" Generated {len(test_questions)} questions\")\n",
        "\n",
        "testset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Using 10 questions for evaluation\n",
            "üìä Evaluating Naive...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be5f32941f534df88f2494fffaa131ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[11]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n",
            "Exception raised in Job[53]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Naive completed - Latency: 5.64s | Cost: $0.0107\n",
            "üìä Evaluating BM25...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e63d08bc646a495ab9fbe71e83867cc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[16]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ BM25 completed - Latency: 5.03s | Cost: $0.0053\n",
            "üìä Evaluating Multi-Query...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "165680ced2d74098a699e00c67ad8fae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[23]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[41]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[11]: AttributeError('StringIO' object has no attribute 'statements')\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[46]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n",
            "Exception raised in Job[53]: TimeoutError()\n",
            "Exception raised in Job[59]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-Query completed - Latency: 6.19s | Cost: $0.0144\n",
            "üìä Evaluating Parent Document...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89885070568b472ab8278960aa72b151",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[59]: AttributeError('StringIO' object has no attribute 'statements')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Parent Document completed - Latency: 3.87s | Cost: $0.0063\n",
            "üìä Evaluating Contextual Compression...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd0ac53f585f44879f0286102fcb4bc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[36]: AttributeError('StringIO' object has no attribute 'classifications')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Contextual Compression completed - Latency: 3.66s | Cost: $0.0042\n",
            "üìä Evaluating Ensemble...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58c016e3ccb64b21aaec538212d3cef6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[11]: TimeoutError()\n",
            "Exception raised in Job[16]: TimeoutError()\n",
            "Exception raised in Job[17]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[34]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n",
            "Exception raised in Job[53]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ensemble completed - Latency: 10.00s | Cost: $0.0074\n",
            "üéâ All evaluations completed!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 4: EVALUATE RETRIEVERS WITH REDUCED QUESTIONS + COST TRACKING (FIXED)\n",
        "# =============================================================================\n",
        "\n",
        "import time\n",
        "from ragas import EvaluationDataset, evaluate\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas import RunConfig\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.callbacks import LangChainTracer\n",
        "from langsmith import Client\n",
        "\n",
        "# Setup evaluation LLM for Ragas\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "# Setup LangSmith tracer for cost tracking\n",
        "langsmith_client = Client()\n",
        "tracer = LangChainTracer(project_name=\"retriever-evaluation\")\n",
        "\n",
        "# Collect existing retrievers from notebook\n",
        "retrievers = {\n",
        "    \"Naive\": naive_retrieval_chain,\n",
        "    \"BM25\": bm25_retrieval_chain,\n",
        "    \"Multi-Query\": multi_query_retrieval_chain,\n",
        "    \"Parent Document\": parent_document_retrieval_chain,\n",
        "    \"Contextual Compression\": contextual_compression_retrieval_chain,\n",
        "    \"Ensemble\": ensemble_retrieval_chain\n",
        "}\n",
        "\n",
        "# Use only first 10 question\n",
        "limited_testset = list(testset)[:10]\n",
        "print(f\"üìä Using {len(limited_testset)} questions for evaluation\")\n",
        "\n",
        "# Evaluate each retriever\n",
        "results = {}\n",
        "for name, chain in retrievers.items():\n",
        "    print(f\"üìä Evaluating {name}...\")\n",
        "    \n",
        "    # Track latency and cost\n",
        "    latencies = []\n",
        "    total_cost = 0.0\n",
        "    eval_dataset_samples = []\n",
        "    \n",
        "    for test_row in limited_testset:\n",
        "        # Measure latency with cost tracking\n",
        "        start_time = time.time()\n",
        "        \n",
        "        result = chain.invoke(\n",
        "            {\"question\": test_row.eval_sample.user_input},\n",
        "            config={\"callbacks\": [tracer], \"tags\": [f\"retriever_{name}\"]}\n",
        "        )\n",
        "        \n",
        "        end_time = time.time()\n",
        "        latencies.append(end_time - start_time)\n",
        "        \n",
        "        # Extract response and context\n",
        "        response = result[\"response\"].content if hasattr(result[\"response\"], 'content') else str(result[\"response\"])\n",
        "        retrieved_contexts = [doc.page_content for doc in result[\"context\"]]\n",
        "        \n",
        "        eval_sample = {\n",
        "            'user_input': test_row.eval_sample.user_input,\n",
        "            'response': response,\n",
        "            'retrieved_contexts': retrieved_contexts,\n",
        "            'reference_contexts': test_row.eval_sample.reference_contexts,\n",
        "            'reference': test_row.eval_sample.reference\n",
        "        }\n",
        "        eval_dataset_samples.append(eval_sample)\n",
        "    \n",
        "    # Create evaluation dataset\n",
        "    import pandas as pd\n",
        "    eval_df = pd.DataFrame(eval_dataset_samples)\n",
        "    evaluation_dataset = EvaluationDataset.from_pandas(eval_df)\n",
        "    \n",
        "    # Run Ragas evaluation\n",
        "    ragas_result = evaluate(\n",
        "        dataset=evaluation_dataset,\n",
        "        metrics=[\n",
        "            LLMContextRecall(), \n",
        "            Faithfulness(), \n",
        "            FactualCorrectness(), \n",
        "            ResponseRelevancy(), \n",
        "            ContextEntityRecall(), \n",
        "            NoiseSensitivity()\n",
        "        ],\n",
        "        llm=evaluator_llm,\n",
        "        run_config=custom_run_config\n",
        "    )\n",
        "    \n",
        "    # üî• FIXED: Convert Decimal to float\n",
        "    tag_filter = f\"has(tags, 'retriever_{name}')\"\n",
        "    recent_runs = list(langsmith_client.list_runs(\n",
        "        project_name=\"retriever-evaluation\",\n",
        "        limit=50,\n",
        "        filter=tag_filter\n",
        "    ))\n",
        "    \n",
        "    for run in recent_runs:\n",
        "        if run.total_cost:\n",
        "            total_cost += float(run.total_cost)  # üî• Convert Decimal to float\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_latency = sum(latencies) / len(latencies)\n",
        "    cost_per_query = total_cost / len(latencies) if len(latencies) > 0 else 0.0\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'ragas_scores': ragas_result,\n",
        "        'avg_latency': avg_latency,\n",
        "        'total_latency': sum(latencies),\n",
        "        'total_cost': total_cost,\n",
        "        'cost_per_query': cost_per_query,\n",
        "        'num_queries': len(latencies)\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ {name} completed - Latency: {avg_latency:.2f}s | Cost: ${total_cost:.4f}\")\n",
        "\n",
        "print(\"üéâ All evaluations completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä RAGAS EVALUATION RESULTS WITH PERFORMANCE METRICS & COST\n",
            "====================================================================================================\n",
            "             Retriever Context Recall Faithfulness Factual Correctness Response Relevancy Context Entity Recall Noise Sensitivity Avg Latency (s) Total Cost ($) Cost per Query ($)  Queries\n",
            "                 Naive          0.817        0.840               0.493              0.849                 0.362             0.430            5.64         0.0107             0.0011       10\n",
            "                  BM25          0.755        0.762               0.474              0.858                 0.375             0.254            5.03         0.0053             0.0005       10\n",
            "           Multi-Query          0.900        0.898               0.382              0.863                 0.513             0.458            6.19         0.0144             0.0014       10\n",
            "       Parent Document          0.777        0.827               0.494              0.939                 0.449             0.447            3.87         0.0063             0.0006       10\n",
            "Contextual Compression          0.776        0.766               0.516              0.756                 0.379             0.303            3.66         0.0042             0.0004       10\n",
            "              Ensemble          1.000        0.975               0.474              0.936                 0.509             0.496           10.00         0.0074             0.0007       10\n",
            "\n",
            "üìà COMPOSITE PERFORMANCE ANALYSIS\n",
            "==================================================\n",
            "Naive                | Quality: 0.750 | Latency: 5.64s | Score/Speed: 0.133\n",
            "BM25                 | Quality: 0.712 | Latency: 5.03s | Score/Speed: 0.142\n",
            "Multi-Query          | Quality: 0.761 | Latency: 6.19s | Score/Speed: 0.123\n",
            "Parent Document      | Quality: 0.759 | Latency: 3.87s | Score/Speed: 0.196\n",
            "Contextual Compression | Quality: 0.703 | Latency: 3.66s | Score/Speed: 0.192\n",
            "Ensemble             | Quality: 0.846 | Latency: 10.00s | Score/Speed: 0.085\n",
            "\n",
            "üí° ANALYSIS:\n",
            "- Higher Quality scores are better\n",
            "- Lower Latency is better\n",
            "- Score/Speed ratio shows quality per second\n",
            "- Consider cost implications for production use\n",
            "- Contextual Compression may have additional API costs (Cohere Rerank)\n",
            "- Multi-Query generates additional LLM calls, increasing cost and latency\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 5: ANALYZE AND DISPLAY RESULTS WITH CORRECTED RAGAS METRICS\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Compile results with Ragas metrics, performance data, and cost\n",
        "summary_data = []\n",
        "for name, result_data in results.items():\n",
        "    ragas_scores = result_data['ragas_scores']\n",
        "    \n",
        "    # üî• CORRECTED: Extract Ragas metric scores from DataFrame\n",
        "    df = ragas_scores.to_pandas()\n",
        "    \n",
        "    # Extract scores using correct column names from the DataFrame\n",
        "    context_recall = df['context_recall'].mean() if 'context_recall' in df.columns else 0.0\n",
        "    faithfulness = df['faithfulness'].mean() if 'faithfulness' in df.columns else 0.0\n",
        "    factual_correctness = df['factual_correctness'].mean() if 'factual_correctness' in df.columns else 0.0\n",
        "    response_relevancy = df['answer_relevancy'].mean() if 'answer_relevancy' in df.columns else 0.0  # Note: answer_relevancy, not response_relevancy\n",
        "    context_entity_recall = df['context_entity_recall'].mean() if 'context_entity_recall' in df.columns else 0.0\n",
        "    noise_sensitivity = df['noise_sensitivity_relevant'].mean() if 'noise_sensitivity_relevant' in df.columns else 0.0  # Note: noise_sensitivity_relevant\n",
        "    \n",
        "    summary_data.append({\n",
        "        'Retriever': name,\n",
        "        'Context Recall': f\"{context_recall:.3f}\",\n",
        "        'Faithfulness': f\"{faithfulness:.3f}\",\n",
        "        'Factual Correctness': f\"{factual_correctness:.3f}\",\n",
        "        'Response Relevancy': f\"{response_relevancy:.3f}\",\n",
        "        'Context Entity Recall': f\"{context_entity_recall:.3f}\",\n",
        "        'Noise Sensitivity': f\"{noise_sensitivity:.3f}\",\n",
        "        'Avg Latency (s)': f\"{result_data['avg_latency']:.2f}\",\n",
        "        'Total Cost ($)': f\"{result_data['total_cost']:.4f}\",\n",
        "        'Cost per Query ($)': f\"{result_data['cost_per_query']:.4f}\",\n",
        "        'Queries': result_data['num_queries']\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(summary_data)\n",
        "\n",
        "print(\"üìä RAGAS EVALUATION RESULTS WITH PERFORMANCE METRICS & COST\")\n",
        "print(\"=\" * 100)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Calculate composite scores for ranking\n",
        "print(\"\\nüìà COMPOSITE PERFORMANCE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, result_data in results.items():\n",
        "    ragas_scores = result_data['ragas_scores']\n",
        "    \n",
        "    # üî• CORRECTED: Extract quality metrics from DataFrame\n",
        "    df = ragas_scores.to_pandas()\n",
        "    \n",
        "    quality_metrics = [\n",
        "        df['faithfulness'].mean() if 'faithfulness' in df.columns else 0.0,\n",
        "        df['factual_correctness'].mean() if 'factual_correctness' in df.columns else 0.0,\n",
        "        df['answer_relevancy'].mean() if 'answer_relevancy' in df.columns else 0.0,  # Note: answer_relevancy\n",
        "        df['context_recall'].mean() if 'context_recall' in df.columns else 0.0\n",
        "    ]\n",
        "    \n",
        "    avg_quality = sum(quality_metrics) / len(quality_metrics) if quality_metrics else 0.0\n",
        "    avg_latency = result_data['avg_latency']\n",
        "    \n",
        "    print(f\"{name:20} | Quality: {avg_quality:.3f} | Latency: {avg_latency:.2f}s | Score/Speed: {avg_quality/avg_latency:.3f}\")\n",
        "\n",
        "print(\"\\nüí° ANALYSIS:\")\n",
        "print(\"- Higher Quality scores are better\")\n",
        "print(\"- Lower Latency is better\") \n",
        "print(\"- Score/Speed ratio shows quality per second\")\n",
        "print(\"- Consider cost implications for production use\")\n",
        "print(\"- Contextual Compression may have additional API costs (Cohere Rerank)\")\n",
        "print(\"- Multi-Query generates additional LLM calls, increasing cost and latency\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä **Recommendation for Loan Complaint Data:**\n",
        "\n",
        "**For this loan complaint dataset, the Parent Document retriever emerges as the optimal choice**, delivering the best overall balance of quality, speed, and cost-effectiveness. Here's why:\n",
        "\n",
        "**Parent Document achieves the highest efficiency** with a Score/Speed ratio of 0.196, combining strong quality (0.759) with excellent response times (3.87s) and reasonable cost ($0.0006 per query). Its standout feature is exceptional response relevancy (0.939) - the highest among all retrievers - which is crucial for loan complaint data where users need precise, contextually appropriate answers about their specific issues.\n",
        "\n",
        "**While Ensemble delivers the highest quality score (0.846)** with perfect context recall, its 10-second response time makes it impractical for customer service applications where users expect quick responses. **Multi-Query offers similar quality (0.761)** but at 60% higher latency and 2.3x the cost, making it inefficient for high-volume deployment.\n",
        "\n",
        "**For loan complaint scenarios**, Parent Document's \"small-to-big\" approach is particularly effective - it searches on focused complaint excerpts but returns full complaint contexts, ensuring users get comprehensive information about similar loan issues while maintaining fast retrieval. The combination of sub-4-second response times, strong factual correctness (0.494), and high response relevancy makes it ideal for production deployment where customer satisfaction depends on both speed and accuracy.\n",
        "\n",
        "**Bottom line**: Parent Document retrieval provides the optimal solution for loan complaint data, maximizing both user experience and operational efficiency."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
